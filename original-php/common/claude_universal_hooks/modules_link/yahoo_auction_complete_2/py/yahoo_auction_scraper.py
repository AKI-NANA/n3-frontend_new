from playwright.sync_api import sync_playwright
import re
import json
import time

def scrape_yahoo_auction(url, debug=True):
    """
    ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ï¼ˆä¾¡æ ¼ãƒã‚°ä¿®æ­£æ¸ˆã¿å®Œæˆç‰ˆï¼‰
    """
    if debug:
        print("ğŸ§ª ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Ÿè¡Œä¸­...")
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            page = browser.new_page()
            
            if debug:
                print(f"ğŸ“„ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿: {url}")
            
            response = page.goto(url, timeout=30000)
            if debug:
                print(f"âœ… HTTP Status: {response.status}")
            
            page.wait_for_timeout(3000)
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title = page.locator('h1').first.text_content()
            if debug:
                print(f"ğŸ“‹ ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...")
            
            # ä¾¡æ ¼å–å¾—ï¼ˆãƒã‚°ä¿®æ­£ç‰ˆï¼‰
            price_element = page.locator('dd:has-text("å††")').first
            price_text = price_element.text_content()
            if debug:
                print(f"ğŸ’° ä¾¡æ ¼ãƒ†ã‚­ã‚¹ãƒˆ: '{price_text}'")
            
            # ä¾¡æ ¼å¤‰æ›ï¼ˆä¿®æ­£ç‰ˆï¼‰- ã€Œ20,000å††ï¼ˆç¨0å††ï¼‰ã€â†’ 20000
            price = 0
            if price_text:
                # æœ€åˆã®ã€Œæ•°å­—,æ•°å­—å††ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
                yen_match = re.search(r'([\d,]+)å††', price_text)
                if yen_match:
                    number_str = yen_match.group(1)  # "20,000"ã‚’å–å¾—
                    price = int(number_str.replace(',', ''))  # 20000ã«å¤‰æ›
                    if debug:
                        print(f"ğŸ’° ä¾¡æ ¼å¤‰æ›: '{number_str}' â†’ {price:,}å††")
                else:
                    if debug:
                        print("ğŸ’° ä¾¡æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # å•†å“èª¬æ˜å–å¾—
            description = "å•†å“èª¬æ˜ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            try:
                divs = page.locator('div').all()
                for div in divs[:50]:
                    text = div.text_content()
                    if text and len(text.strip()) > 100:
                        description = text.strip()[:300] + "..."
                        break
            except:
                pass
            
            # ç”»åƒURLå–å¾—
            image_urls = []
            try:
                imgs = page.locator('img').all()
                for img in imgs:
                    src = img.get_attribute('src')
                    if src and ('auction' in src or 'yahoo' in src):
                        image_urls.append(src)
            except:
                pass
            
            # çµæœæ§‹ç¯‰
            result = {
                'url': url,
                'title_jp': title,
                'price_jpy': price,
                'price_text': price_text,
                'description_jp': description,
                'image_urls': '|'.join(image_urls[:10]),
                'scrape_success': True,
                'scrape_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            if debug:
                print("\nğŸ“Š æœ€çµ‚çµæœ:")
                print(json.dumps(result, ensure_ascii=False, indent=2))
            
            page.wait_for_timeout(2000)
            browser.close()
            
            return result
            
    except Exception as e:
        if debug:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'url': url,
            'scrape_success': False,
            'error': str(e),
            'scrape_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨
def test():
    url = "https://auctions.yahoo.co.jp/jp/auction/p1198293948"
    result = scrape_yahoo_auction(url)
    
    if result['scrape_success']:
        print(f"\nğŸ‰ æˆåŠŸï¼ä¾¡æ ¼: {result['price_jpy']:,}å††ï¼ˆæ­£ã—ã2ä¸‡å††ï¼‰")
    else:
        print(f"\nâŒ å¤±æ•—: {result.get('error')}")

if __name__ == "__main__":
    test()
