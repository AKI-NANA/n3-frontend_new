#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ Yahoo ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆURLè¨­å®šä¿®æ­£ç‰ˆï¼‰
ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰URLæ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ãƒ»ãƒ‡ãƒãƒƒã‚°å¼·åŒ–
"""

import sys
import json
import time
import re
import urllib.parse
from datetime import datetime
import traceback

def log_message(level, message):
    """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡ºåŠ›"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {level}: {message}")

def validate_yahoo_url(url):
    """Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³URLæ¤œè¨¼ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    if not url:
        return False, "URLãŒç©ºã§ã™"
    
    if not isinstance(url, str):
        return False, "URLãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šå…¥åŠ›URLè©³ç´°æƒ…å ±
    log_message("DEBUG", f"å…¥åŠ›URLè©³ç´°: {repr(url)}")
    log_message("DEBUG", f"URLé•·ã•: {len(url)}")
    log_message("DEBUG", f"URLã‚¿ã‚¤ãƒ—: {type(url)}")
    
    # URLã®å‰å¾Œç©ºç™½é™¤å»
    url = url.strip()
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ›ã‚¹ãƒˆURLæ¤œå‡ºï¼ˆã‚¨ãƒ©ãƒ¼ã®åŸå› ï¼‰
    localhost_patterns = [
        r'https?://localhost',
        r'https?://127\.0\.0\.1',
        r'https?://.*\.local',
        r'file://'
    ]
    
    for pattern in localhost_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            return False, f"ãƒ­ãƒ¼ã‚«ãƒ«URLã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: {url[:50]}"
    
    # Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³URLãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç·©å’Œç‰ˆï¼‰
    yahoo_patterns = [
        # æ¨™æº–çš„ãªYahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³URL
        r'https?://auctions\.yahoo\.co\.jp/jp/auction/[a-zA-Z0-9]+',
        r'https?://page\.auctions\.yahoo\.co\.jp/jp/auction/[a-zA-Z0-9]+',
        
        # è¿½åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³
        r'https?://.*auctions\.yahoo\.co\.jp.*auction.*',
        r'https?://.*yahoo\.co\.jp.*auction.*',
        
        # ãƒ¢ãƒã‚¤ãƒ«ç‰ˆ
        r'https?://.*yahoo\.co\.jp/.*auction.*',
        
        # çŸ­ç¸®URLã‚„ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆçµŒç”±ã®å¯èƒ½æ€§ã‚‚è€ƒæ…®
        r'https?://.*yahoo\..*auction.*'
    ]
    
    for pattern in yahoo_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            # è¿½åŠ ãƒã‚§ãƒƒã‚¯ï¼šauction IDã®å­˜åœ¨ç¢ºèª
            if re.search(r'auction[/=]([a-zA-Z0-9]+)', url):
                return True, "æœ‰åŠ¹ãªYahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³URL"
    
    # ãƒ†ã‚¹ãƒˆç”¨URLè¨±å¯ï¼ˆé–‹ç™ºæ™‚ã®ã¿ï¼‰
    test_patterns = [
        r'https?://.*yahoo.*test.*',
        r'https://auctions\.yahoo\.co\.jp/jp/auction/test\d+'
    ]
    
    for pattern in test_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            log_message("WARNING", "ãƒ†ã‚¹ãƒˆç”¨URLã‚’æ¤œå‡º - é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†ã—ã¾ã™")
            return True, "ãƒ†ã‚¹ãƒˆç”¨Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³URL"
    
    # è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if 'yahoo' in url.lower():
        if 'auction' not in url.lower():
            return False, f"Yahoo URLã§ã™ãŒã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {url[:50]}"
        else:
            return False, f"Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³URLã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {url[:50]}"
    
    return False, f"Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ä»¥å¤–ã®URLã§ã™ã€‚æ­£ã—ã„Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: {url[:50]}"

def extract_auction_id(url):
    """ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³IDæŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    patterns = [
        r'/auction/([a-zA-Z0-9]+)',
        r'auctionID=([a-zA-Z0-9]+)',
        r'auction[/_=]([a-zA-Z0-9]+)',
        r'/([a-zA-Z0-9]+)(?:\?|$)'  # URLæœ«å°¾ã®ID
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            auction_id = match.group(1)
            # IDã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if len(auction_id) > 3 and auction_id.isalnum():
                return auction_id
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šURLã®æœ€å¾Œã®éƒ¨åˆ†ã‚’ä½¿ç”¨
    parts = url.rstrip('/').split('/')
    for part in reversed(parts):
        if part and len(part) > 3 and part.isalnum():
            return part
    
    return "unknown"

def check_dependencies():
    """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
    required_modules = ['playwright', 'psycopg2', 'pandas']
    missing_modules = []
    
    for module in required_modules:
        try:
            __import__(module)
            log_message("INFO", f"âœ… {module} ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèªå®Œäº†")
        except ImportError:
            missing_modules.append(module)
            log_message("WARNING", f"âš ï¸ {module} ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    return len(missing_modules) == 0, missing_modules

def simple_scraping_simulation(url):
    """ç°¡æ˜“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    log_message("INFO", "ğŸ“‹ ç°¡æ˜“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
    
    # URLã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    auction_id = extract_auction_id(url)
    
    # URLã«å¿œã˜ãŸãƒªã‚¢ãƒ«ãªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    if 'test' in url.lower():
        sample_data = {
            'auction_id': auction_id,
            'url': url,
            'title': f'ã€ãƒ†ã‚¹ãƒˆå•†å“ã€‘iPhone 14 Pro 128GB ã‚¹ãƒšãƒ¼ã‚¹ãƒ–ãƒ©ãƒƒã‚¯ SIMãƒ•ãƒªãƒ¼_{auction_id}',
            'price': 89800,
            'currency': 'JPY',
            'description': 'ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ™‚ã«ã¯æœ¬ç‰©ã®å•†å“æƒ…å ±ãŒå–å¾—ã•ã‚Œã¾ã™ã€‚',
            'condition': 'Used',
            'category': 'ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³æœ¬ä½“',
            'images': [
                'https://auctions.c.yimg.jp/images.auctions.yahoo.co.jp/image/test1.jpg',
                'https://auctions.c.yimg.jp/images.auctions.yahoo.co.jp/image/test2.jpg'
            ],
            'seller_info': 'test_seller_001',
            'status': 'simulation',
            'scraped_at': datetime.now().isoformat(),
            'note': 'ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ã™'
        }
    else:
        sample_data = {
            'auction_id': auction_id,
            'url': url,
            'title': f'ã‚µãƒ³ãƒ—ãƒ«å•†å“ãƒ‡ãƒ¼ã‚¿_{auction_id}',
            'price': 29800,
            'currency': 'JPY',
            'description': 'ã“ã‚Œã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚',
            'condition': 'New',
            'category': 'Electronics',
            'images': [
                'https://via.placeholder.com/600x400/0066cc/white?text=Sample+Image+1',
                'https://via.placeholder.com/600x400/cc6600/white?text=Sample+Image+2'
            ],
            'seller_info': 'sample_seller',
            'status': 'simulation',
            'scraped_at': datetime.now().isoformat()
        }
    
    log_message("SUCCESS", f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {sample_data['title']}")
    return sample_data

def provide_url_examples():
    """æ­£ã—ã„URLä¾‹ã®æä¾›"""
    examples = [
        "https://auctions.yahoo.co.jp/jp/auction/abc123456",
        "https://page.auctions.yahoo.co.jp/jp/auction/xyz789012",
        "https://auctions.yahoo.co.jp/jp/auction/test123 (ãƒ†ã‚¹ãƒˆç”¨)"
    ]
    
    return {
        'valid_examples': examples,
        'format_explanation': 'Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ã®URLã¯ã€Œhttps://auctions.yahoo.co.jp/jp/auction/å•†å“IDã€ã®å½¢å¼ã§ã™',
        'how_to_get': 'Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ã§å•†å“ãƒšãƒ¼ã‚¸ã‚’é–‹ãã€ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒãƒ¼ã‹ã‚‰URLã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„'
    }

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    log_message("INFO", "ğŸš€ Yahoo ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ï¼ˆURLä¿®æ­£ç‰ˆï¼‰")
    
    try:
        # å¼•æ•°ãƒã‚§ãƒƒã‚¯
        if len(sys.argv) < 2:
            log_message("ERROR", "âŒ URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            url_examples = provide_url_examples()
            print(json.dumps({
                'success': False,
                'error': 'URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“',
                'usage': 'python scraping_system_fixed.py <Yahoo_Auction_URL>',
                'examples': url_examples
            }, ensure_ascii=False, indent=2))
            sys.exit(1)
        
        url = sys.argv[1]
        log_message("INFO", f"ğŸ“¥ å¯¾è±¡URL: {url}")
        
        # URLæ¤œè¨¼ï¼ˆä¿®æ­£ç‰ˆï¼‰
        is_valid, validation_message = validate_yahoo_url(url)
        log_message("INFO", f"ğŸ” URLæ¤œè¨¼çµæœ: {validation_message}")
        
        if not is_valid:
            url_examples = provide_url_examples()
            print(json.dumps({
                'success': False,
                'error': f'ç„¡åŠ¹ãªURL: {validation_message}',
                'url': url,
                'examples': url_examples,
                'fix_suggestion': 'Yahoo ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ã®å•†å“ãƒšãƒ¼ã‚¸URLã‚’æ­£ã—ãå…¥åŠ›ã—ã¦ãã ã•ã„'
            }, ensure_ascii=False, indent=2))
            sys.exit(1)
        
        # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
        deps_ok, missing = check_dependencies()
        
        if not deps_ok:
            log_message("WARNING", f"âš ï¸ ä¸è¶³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {missing}")
            log_message("INFO", "ğŸ”„ ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ")
            
            # ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
            result = simple_scraping_simulation(url)
            
            print(json.dumps({
                'success': True,
                'mode': 'simulation',
                'message': 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†',
                'data': result,
                'missing_modules': missing,
                'note': 'æœ¬æ ¼çš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ã¯ pip install playwright psycopg2-binary pandas ãŒå¿…è¦ã§ã™'
            }, ensure_ascii=False, indent=2))
            
        else:
            # æœ¬æ ¼çš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            log_message("INFO", "ğŸ”¥ æœ¬æ ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ")
            
            try:
                # å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ï¼ˆä¾å­˜é–¢ä¿‚ãŒæ•´ã£ã¦ã„ã‚‹å ´åˆï¼‰
                result = perform_real_scraping(url)
                
                print(json.dumps({
                    'success': True,
                    'mode': 'real_scraping',
                    'message': 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†',
                    'data': result
                }, ensure_ascii=False, indent=2))
                
            except Exception as e:
                log_message("ERROR", f"âŒ æœ¬æ ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
                result = simple_scraping_simulation(url)
                
                print(json.dumps({
                    'success': True,
                    'mode': 'fallback_simulation',
                    'message': 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†',
                    'data': result,
                    'original_error': str(e)
                }, ensure_ascii=False, indent=2))
        
        log_message("SUCCESS", "âœ… å‡¦ç†å®Œäº†")
        
    except Exception as e:
        log_message("ERROR", f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        log_message("ERROR", f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°:\n{traceback.format_exc()}")
        
        print(json.dumps({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'timestamp': datetime.now().isoformat(),
            'examples': provide_url_examples()
        }, ensure_ascii=False, indent=2))
        
        sys.exit(1)

def perform_real_scraping(url):
    """å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ï¼ˆPlaywrightä½¿ç”¨ï¼‰"""
    from playwright.sync_api import sync_playwright
    
    log_message("INFO", "ğŸ­ Playwrightèµ·å‹•ä¸­...")
    
    with sync_playwright() as p:
        # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        # User-Agentè¨­å®š
        page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        log_message("INFO", f"ğŸŒ ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹: {url}")
        
        # ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
        response = page.goto(url, timeout=30000)
        log_message("INFO", f"ğŸ“¡ HTTP Status: {response.status}")
        
        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
        page.wait_for_timeout(3000)
        
        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        title = extract_title(page)
        price = extract_price(page)
        description = extract_description(page)
        images = extract_images(page)
        
        browser.close()
        
        return {
            'auction_id': extract_auction_id(url),
            'url': url,
            'title': title,
            'price': price,
            'description': description[:500],  # 500æ–‡å­—åˆ¶é™
            'images': images[:5],  # 5æšåˆ¶é™
            'status': 'scraped',
            'scraped_at': datetime.now().isoformat()
        }

def extract_title(page):
    """ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
    selectors = ['h1', '.ProductTitle__text', '[data-cl-params*="title"]']
    
    for selector in selectors:
        try:
            element = page.locator(selector).first
            if element.is_visible():
                title = element.text_content().strip()
                if title and len(title) > 5:
                    return title
        except:
            continue
    
    return page.title() or "ã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—"

def extract_price(page):
    """ä¾¡æ ¼æŠ½å‡º"""
    selectors = ['dd:has-text("å††")', '.Price--bid', '.ProductPrice dd']
    
    for selector in selectors:
        try:
            element = page.locator(selector).first
            if element.is_visible():
                price_text = element.text_content().strip()
                match = re.search(r'([\d,]+)å††', price_text)
                if match:
                    return int(match.group(1).replace(',', ''))
        except:
            continue
    
    return 0

def extract_description(page):
    """èª¬æ˜æŠ½å‡º"""
    selectors = ['.ProductExplanation__commentArea', '.ProductDescription__body']
    
    for selector in selectors:
        try:
            element = page.locator(selector).first
            if element.is_visible():
                desc = element.text_content().strip()
                if desc and len(desc) > 30:
                    return desc
        except:
            continue
    
    return "å•†å“èª¬æ˜å–å¾—å¤±æ•—"

def extract_images(page):
    """ç”»åƒæŠ½å‡º"""
    selectors = ['.ProductImage img', 'img[src*="auctions.c.yimg.jp"]']
    
    images = []
    for selector in selectors:
        try:
            imgs = page.locator(selector).all()
            for img in imgs:
                src = img.get_attribute('src')
                if src and 'auctions.c.yimg.jp' in src:
                    images.append(src)
        except:
            continue
    
    return list(dict.fromkeys(images))  # é‡è¤‡å‰Šé™¤

if __name__ == "__main__":
    main()
