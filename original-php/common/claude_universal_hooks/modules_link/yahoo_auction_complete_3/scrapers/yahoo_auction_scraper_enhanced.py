#!/usr/bin/env python3
"""
Yahoo Auction ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« - ä¿®æ­£ç‰ˆ
ãƒ¢ãƒ€ãƒ³ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æŠ€è¡“ã‚’ä½¿ç”¨ã—ãŸå®‰å®šç‰ˆ
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import random
from urllib.parse import urljoin, urlparse
import logging
from datetime import datetime
import sqlite3
import os

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class YahooAuctionScraper:
    def __init__(self):
        self.session = requests.Session()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        self.database_path = 'yahoo_auction_scraped_data.db'
        self.init_database()
    
    def init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        try:
            conn = sqlite3.connect(self.database_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS scraped_products (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    auction_id TEXT UNIQUE,
                    title TEXT,
                    current_price TEXT,
                    buy_now_price TEXT,
                    shipping_cost TEXT,
                    end_time TEXT,
                    condition_text TEXT,
                    seller_name TEXT,
                    bid_count INTEGER,
                    watch_count INTEGER,
                    main_image_url TEXT,
                    additional_images TEXT,
                    description TEXT,
                    category TEXT,
                    source_url TEXT,
                    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def extract_auction_id(self, url):
        """URLã‹ã‚‰ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³IDã‚’æŠ½å‡º"""
        try:
            if 'auctions.yahoo.co.jp' in url and '/auction/' in url:
                return url.split('/auction/')[-1].split('?')[0]
            return None
        except Exception as e:
            logger.error(f"ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³IDæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def scrape_auction_page(self, url):
        """å˜ä¸€ã®ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³IDå–å¾—
            auction_id = self.extract_auction_id(url)
            if not auction_id:
                logger.warning(f"ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {url}")
                return None
            
            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            data = {
                'auction_id': auction_id,
                'source_url': url,
                'title': self.extract_title(soup),
                'current_price': self.extract_current_price(soup),
                'buy_now_price': self.extract_buy_now_price(soup),
                'shipping_cost': self.extract_shipping_cost(soup),
                'end_time': self.extract_end_time(soup),
                'condition_text': self.extract_condition(soup),
                'seller_name': self.extract_seller(soup),
                'bid_count': self.extract_bid_count(soup),
                'watch_count': self.extract_watch_count(soup),
                'main_image_url': self.extract_main_image(soup),
                'additional_images': self.extract_additional_images(soup),
                'description': self.extract_description(soup, url),
                'category': self.extract_category(soup)
            }
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {data['title'][:50]}...")
            return data
            
        except requests.exceptions.RequestException as e:
            logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
        except Exception as e:
            logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_title(self, soup):
        """å•†å“ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        try:
            selectors = [
                'h1.ProductTitle__text',
                'h1[data-testid="product-title"]',
                '.ProductTitle h1',
                'h1'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
            
            return 'ã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—'
        except Exception as e:
            logger.error(f"ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'ã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—'
    
    def extract_current_price(self, soup):
        """ç¾åœ¨ä¾¡æ ¼æŠ½å‡º"""
        try:
            selectors = [
                '.Price__value',
                '.ProductPrice__value',
                '[data-testid="current-price"]',
                '.price .value'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
            
            return 'ä¾¡æ ¼ä¸æ˜'
        except Exception as e:
            logger.error(f"ä¾¡æ ¼æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'ä¾¡æ ¼ä¸æ˜'
    
    def extract_buy_now_price(self, soup):
        """å³æ±ºä¾¡æ ¼æŠ½å‡º"""
        try:
            selectors = [
                '.ProductPrice__price--fixed .ProductPrice__value',
                '[data-testid="buy-now-price"]',
                '.buy-now-price .value'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
            
            return 'å³æ±ºä¾¡æ ¼ãªã—'
        except Exception as e:
            logger.error(f"å³æ±ºä¾¡æ ¼æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'å³æ±ºä¾¡æ ¼ãªã—'
    
    def extract_shipping_cost(self, soup):
        """é€æ–™æŠ½å‡º"""
        try:
            selectors = [
                '.Shipping .Price__value',
                '[data-testid="shipping-cost"]',
                '.shipping-cost'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
            
            return 'é€æ–™ä¸æ˜'
        except Exception as e:
            logger.error(f"é€æ–™æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'é€æ–™ä¸æ˜'
    
    def extract_end_time(self, soup):
        """çµ‚äº†æ™‚é–“æŠ½å‡º"""
        try:
            selectors = [
                '.EndTime__value',
                '[data-testid="end-time"]',
                '.end-time'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
            
            return 'çµ‚äº†æ™‚é–“ä¸æ˜'
        except Exception as e:
            logger.error(f"çµ‚äº†æ™‚é–“æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'çµ‚äº†æ™‚é–“ä¸æ˜'
    
    def extract_condition(self, soup):
        """å•†å“çŠ¶æ…‹æŠ½å‡º"""
        try:
            selectors = [
                '.Condition__value',
                '[data-testid="condition"]',
                '.condition'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
            
            return 'çŠ¶æ…‹ä¸æ˜'
        except Exception as e:
            logger.error(f"çŠ¶æ…‹æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'çŠ¶æ…‹ä¸æ˜'
    
    def extract_seller(self, soup):
        """å‡ºå“è€…æŠ½å‡º"""
        try:
            selectors = [
                '.Seller__name',
                '[data-testid="seller-name"]',
                '.seller-name'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
            
            return 'å‡ºå“è€…ä¸æ˜'
        except Exception as e:
            logger.error(f"å‡ºå“è€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'å‡ºå“è€…ä¸æ˜'
    
    def extract_bid_count(self, soup):
        """å…¥æœ­æ•°æŠ½å‡º"""
        try:
            selectors = [
                '.BidCount__value',
                '[data-testid="bid-count"]',
                '.bid-count'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    # æ•°å­—ã®ã¿æŠ½å‡º
                    import re
                    numbers = re.findall(r'\d+', text)
                    return int(numbers[0]) if numbers else 0
            
            return 0
        except Exception as e:
            logger.error(f"å…¥æœ­æ•°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def extract_watch_count(self, soup):
        """ã‚¦ã‚©ãƒƒãƒæ•°æŠ½å‡º"""
        try:
            selectors = [
                '.WatchCount__value',
                '[data-testid="watch-count"]',
                '.watch-count'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    # æ•°å­—ã®ã¿æŠ½å‡º
                    import re
                    numbers = re.findall(r'\d+', text)
                    return int(numbers[0]) if numbers else 0
            
            return 0
        except Exception as e:
            logger.error(f"ã‚¦ã‚©ãƒƒãƒæ•°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def extract_main_image(self, soup):
        """ãƒ¡ã‚¤ãƒ³ç”»åƒURLæŠ½å‡º"""
        try:
            selectors = [
                '.ProductImage__main img',
                '.product-image img',
                '.main-image img',
                'img[alt*="å•†å“"]',
                'img'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element and element.get('src'):
                    src = element.get('src')
                    if 'auctions.c.yimg.jp' in src or 'yahoo' in src:
                        return src
            
            return 'https://via.placeholder.com/300x300?text=No+Image'
        except Exception as e:
            logger.error(f"ãƒ¡ã‚¤ãƒ³ç”»åƒæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'https://via.placeholder.com/300x300?text=No+Image'
    
    def extract_additional_images(self, soup):
        """è¿½åŠ ç”»åƒURLæŠ½å‡º"""
        try:
            images = []
            selectors = [
                '.ProductImage__thumbnail img',
                '.thumbnail img',
                '.additional-images img'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                for element in elements:
                    src = element.get('src') or element.get('data-src')
                    if src and ('auctions.c.yimg.jp' in src or 'yahoo' in src):
                        images.append(src)
            
            return json.dumps(images[:10])  # æœ€å¤§10æš
        except Exception as e:
            logger.error(f"è¿½åŠ ç”»åƒæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return json.dumps([])
    
    def extract_description(self, soup, url):
        """å•†å“èª¬æ˜æŠ½å‡º"""
        try:
            # iframeå†…ã®èª¬æ˜ã‚’å–å¾—è©¦è¡Œ
            iframe = soup.select_one('iframe#description_iframe')
            if iframe and iframe.get('src'):
                try:
                    iframe_url = urljoin(url, iframe.get('src'))
                    iframe_response = self.session.get(iframe_url, timeout=15)
                    iframe_soup = BeautifulSoup(iframe_response.text, 'html.parser')
                    description = iframe_soup.get_text(strip=True)
                    if description and len(description) > 10:
                        return description[:2000]  # 2000æ–‡å­—ã¾ã§
                except Exception as e:
                    logger.warning(f"iframeå–å¾—å¤±æ•—: {e}")
            
            # é€šå¸¸ã®èª¬æ˜æ–‡å–å¾—
            selectors = [
                '.ProductExplanation__body',
                '.product-explanation',
                '.description',
                '[data-testid="description"]'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    if text and len(text) > 10:
                        return text[:2000]  # 2000æ–‡å­—ã¾ã§
            
            return 'èª¬æ˜æ–‡ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ'
        except Exception as e:
            logger.error(f"èª¬æ˜æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'èª¬æ˜æ–‡ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ'
    
    def extract_category(self, soup):
        """ã‚«ãƒ†ã‚´ãƒªæŠ½å‡º"""
        try:
            selectors = [
                '.Breadcrumb a',
                '.breadcrumb a',
                '.category-path a',
                '[data-testid="breadcrumb"] a'
            ]
            
            categories = []
            for selector in selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text(strip=True)
                    if text and text not in ['ãƒ›ãƒ¼ãƒ ', 'Yahoo!ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³']:
                        categories.append(text)
            
            return ' > '.join(categories[:3]) if categories else 'ã‚«ãƒ†ã‚´ãƒªä¸æ˜'
        except Exception as e:
            logger.error(f"ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return 'ã‚«ãƒ†ã‚´ãƒªä¸æ˜'
    
    def save_to_database(self, data):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            conn = sqlite3.connect(self.database_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO scraped_products 
                (auction_id, title, current_price, buy_now_price, shipping_cost,
                 end_time, condition_text, seller_name, bid_count, watch_count,
                 main_image_url, additional_images, description, category, source_url)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                data['auction_id'], data['title'], data['current_price'],
                data['buy_now_price'], data['shipping_cost'], data['end_time'],
                data['condition_text'], data['seller_name'], data['bid_count'],
                data['watch_count'], data['main_image_url'], data['additional_images'],
                data['description'], data['category'], data['source_url']
            ))
            
            conn.commit()
            conn.close()
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜å®Œäº†: {data['auction_id']}")
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def scrape_multiple_urls(self, urls):
        """è¤‡æ•°URLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        results = []
        
        for i, url in enumerate(urls):
            try:
                logger.info(f"å‡¦ç†ä¸­ {i+1}/{len(urls)}: {url}")
                
                data = self.scrape_auction_page(url)
                if data:
                    self.save_to_database(data)
                    results.append(data)
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                if i < len(urls) - 1:
                    wait_time = random.uniform(2, 5)
                    logger.info(f"å¾…æ©Ÿä¸­: {wait_time:.1f}ç§’")
                    time.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"URLå‡¦ç†ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                continue
        
        return results
    
    def export_to_csv(self, filename=None):
        """CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            if not filename:
                filename = f'yahoo_auction_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
            
            conn = sqlite3.connect(self.database_path)
            df = pd.read_sql_query("SELECT * FROM scraped_products ORDER BY scraped_at DESC", conn)
            conn.close()
            
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            logger.info(f"CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def generate_html_report(self, data_list, filename=None):
        """HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            if not filename:
                filename = f'yahoo_auction_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.html'
            
            html_content = '''
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yahoo Auction ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ</title>
    <link href="https://cdn.tailwindcss.com/2.2.19/tailwind.min.css" rel="stylesheet">
    <style>
        body { font-family: 'Hiragino Sans', 'Yu Gothic', sans-serif; }
        .product-card { transition: all 0.3s ease; }
        .product-card:hover { transform: translateY(-2px); box-shadow: 0 8px 25px rgba(0,0,0,0.15); }
        .image-gallery img { border-radius: 8px; }
    </style>
</head>
<body class="bg-gray-50">
    <div class="container mx-auto p-6">
        <div class="bg-white rounded-lg shadow-lg p-6 mb-8">
            <h1 class="text-3xl font-bold text-gray-800 mb-4">Yahoo Auction ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ</h1>
            <div class="grid grid-cols-1 md:grid-cols-3 gap-4">
                <div class="bg-blue-50 p-4 rounded-lg">
                    <h3 class="text-lg font-semibold text-blue-800">å–å¾—ä»¶æ•°</h3>
                    <p class="text-2xl font-bold text-blue-600">{total_count}ä»¶</p>
                </div>
                <div class="bg-green-50 p-4 rounded-lg">
                    <h3 class="text-lg font-semibold text-green-800">å¹³å‡ä¾¡æ ¼</h3>
                    <p class="text-2xl font-bold text-green-600">è¨ˆç®—ä¸­</p>
                </div>
                <div class="bg-purple-50 p-4 rounded-lg">
                    <h3 class="text-lg font-semibold text-purple-800">å–å¾—æ—¥æ™‚</h3>
                    <p class="text-lg font-semibold text-purple-600">{timestamp}</p>
                </div>
            </div>
        </div>
        
        <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
            {product_cards}
        </div>
    </div>
</body>
</html>
            '''
            
            product_cards = ''
            for data in data_list:
                additional_images = json.loads(data.get('additional_images', '[]'))
                image_gallery = ''
                
                for img_url in additional_images[:6]:
                    image_gallery += f'<img src="{img_url}" alt="è¿½åŠ ç”»åƒ" class="w-16 h-16 object-cover rounded border">'
                
                product_cards += f'''
                <div class="product-card bg-white rounded-lg shadow-md p-6">
                    <div class="flex items-start space-x-4">
                        <img src="{data.get('main_image_url', '')}" alt="å•†å“ç”»åƒ" 
                             class="w-24 h-24 object-cover rounded-lg border">
                        <div class="flex-1">
                            <h3 class="text-lg font-semibold text-gray-800 mb-2">{data.get('title', '')[:80]}...</h3>
                            <div class="grid grid-cols-2 gap-2 text-sm">
                                <div><span class="font-medium">ç¾åœ¨ä¾¡æ ¼:</span> <span class="text-red-600 font-bold">{data.get('current_price', '')}</span></div>
                                <div><span class="font-medium">å³æ±ºä¾¡æ ¼:</span> <span class="text-green-600 font-bold">{data.get('buy_now_price', '')}</span></div>
                                <div><span class="font-medium">é€æ–™:</span> {data.get('shipping_cost', '')}</div>
                                <div><span class="font-medium">å…¥æœ­:</span> {data.get('bid_count', 0)}ä»¶</div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="mt-4">
                        <p class="text-sm text-gray-600 mb-2"><span class="font-medium">å‡ºå“è€…:</span> {data.get('seller_name', '')}</p>
                        <p class="text-sm text-gray-600 mb-2"><span class="font-medium">çŠ¶æ…‹:</span> {data.get('condition_text', '')}</p>
                        <p class="text-sm text-gray-600 mb-3"><span class="font-medium">ã‚«ãƒ†ã‚´ãƒª:</span> {data.get('category', '')}</p>
                        
                        {f'<div class="image-gallery flex space-x-2 mb-3">{image_gallery}</div>' if image_gallery else ''}
                        
                        <div class="bg-gray-50 p-3 rounded">
                            <p class="text-xs text-gray-700">{data.get('description', '')[:200]}...</p>
                        </div>
                        
                        <div class="mt-3 flex justify-between items-center">
                            <span class="text-xs text-gray-500">ID: {data.get('auction_id', '')}</span>
                            <a href="{data.get('source_url', '')}" target="_blank" 
                               class="bg-blue-500 text-white px-3 py-1 rounded text-xs hover:bg-blue-600">
                                è©³ç´°ã‚’è¦‹ã‚‹
                            </a>
                        </div>
                    </div>
                </div>
                '''
            
            final_html = html_content.format(
                total_count=len(data_list),
                timestamp=datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M'),
                product_cards=product_cards
            )
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(final_html)
            
            logger.info(f"HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = YahooAuctionScraper()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URLï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
    sample_urls = [
        'https://auctions.yahoo.co.jp/jp/auction/w1190447053',
        'https://auctions.yahoo.co.jp/jp/auction/f1140003045',
        # å®Ÿéš›ã®URLã«ç½®ãæ›ãˆã¦ãã ã•ã„
    ]
    
    print("ğŸš€ Yahoo Auction ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
    print(f"å¯¾è±¡URLæ•°: {len(sample_urls)}")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    results = scraper.scrape_multiple_urls(sample_urls)
    
    if results:
        print(f"\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(results)}ä»¶")
        
        # CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        csv_file = scraper.export_to_csv()
        if csv_file:
            print(f"ğŸ“Š CSVãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {csv_file}")
        
        # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_file = scraper.generate_html_report(results)
        if html_file:
            print(f"ğŸ“„ HTMLãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {html_file}")
            print(f"ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ã§ {html_file} ã‚’é–‹ã„ã¦çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    else:
        print("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
