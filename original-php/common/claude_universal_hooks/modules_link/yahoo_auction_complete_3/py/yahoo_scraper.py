from playwright.sync_api import sync_playwright
import re
import json
import time
import sys

def scrape_yahoo_auction(url, debug=False):
    """
    ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ï¼ˆN3çµ±åˆç‰ˆï¼‰
    """
    if debug:
        print("ðŸ§ª ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Ÿè¡Œä¸­...", file=sys.stderr)
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)  # Webã§ã¯Headless
            page = browser.new_page()
            
            if debug:
                print(f"ðŸ“„ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿: {url}", file=sys.stderr)
            
            response = page.goto(url, timeout=30000)
            if debug:
                print(f"âœ… HTTP Status: {response.status}", file=sys.stderr)
            
            page.wait_for_timeout(3000)
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title = page.locator('h1').first.text_content()
            if debug:
                print(f"ðŸ“‹ ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...", file=sys.stderr)
            
            # ä¾¡æ ¼å–å¾—ï¼ˆãƒã‚°ä¿®æ­£ç‰ˆï¼‰
            price_element = page.locator('dd:has-text("å††")').first
            price_text = price_element.text_content()
            if debug:
                print(f"ðŸ’° ä¾¡æ ¼ãƒ†ã‚­ã‚¹ãƒˆ: '{price_text}'", file=sys.stderr)
            
            # ä¾¡æ ¼å¤‰æ›ï¼ˆä¿®æ­£ç‰ˆï¼‰
            price = 0
            if price_text:
                yen_match = re.search(r'([\d,]+)å††', price_text)
                if yen_match:
                    number_str = yen_match.group(1)
                    price = int(number_str.replace(',', ''))
                    if debug:
                        print(f"ðŸ’° ä¾¡æ ¼å¤‰æ›: '{number_str}' â†’ {price:,}å††", file=sys.stderr)
                else:
                    if debug:
                        print("ðŸ’° ä¾¡æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", file=sys.stderr)
            
            # å•†å“èª¬æ˜Žå–å¾—
            description = "å•†å“èª¬æ˜ŽãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            try:
                divs = page.locator('div').all()
                for div in divs[:50]:
                    text = div.text_content()
                    if text and len(text.strip()) > 100:
                        description = text.strip()[:300] + "..."
                        break
            except:
                pass
            
            # ç”»åƒURLå–å¾—
            image_urls = []
            try:
                imgs = page.locator('img').all()
                for img in imgs:
                    src = img.get_attribute('src')
                    if src and ('auction' in src or 'yahoo' in src):
                        image_urls.append(src)
            except:
                pass
            
            # çµæžœæ§‹ç¯‰
            result = {
                'url': url,
                'title_jp': title,
                'price_jpy': price,
                'price_text': price_text,
                'description_jp': description,
                'image_urls': '|'.join(image_urls[:10]),
                'scrape_success': True,
                'scrape_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            if debug:
                print("\nðŸ“Š æœ€çµ‚çµæžœ:", file=sys.stderr)
                print(json.dumps(result, ensure_ascii=False, indent=2), file=sys.stderr)
            
            browser.close()
            return result
            
    except Exception as e:
        if debug:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return {
            'url': url,
            'scrape_success': False,
            'error': str(e),
            'scrape_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }

if __name__ == "__main__":
    if len(sys.argv) > 1:
        url = sys.argv[1]
        result = scrape_yahoo_auction(url, debug=True)
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        print("Usage: python yahoo_scraper.py <yahoo_auction_url>")
