#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ Yahoo ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° + PostgreSQLçµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨ç‰ˆï¼‰
çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¯¾å¿œãƒ»eBay APIé€£æºæº–å‚™ãƒ»é‡è¤‡é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ 
"""

from playwright.sync_api import sync_playwright
import psycopg2
import psycopg2.extras
import pandas as pd
import re
import json
import time
import uuid
import hashlib
import os
from datetime import datetime
from pathlib import Path
from decimal import Decimal

class UnifiedScrapingSystem:
    def __init__(self, db_config=None):
        """çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        
        # PostgreSQLæ¥ç¶šè¨­å®š
        self.db_config = db_config or {
            'host': 'localhost',
            'database': 'nagano3_db',
            'user': 'aritahiroaki',
            'password': '',
            'port': 5432
        }
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        self.session_id = f"SCRAPE_{int(time.time())}"
        self.session_stats = {
            'total_urls': 0,
            'successful_scrapes': 0,
            'failed_scrapes': 0,
            'duplicates_skipped': 0,
            'errors': []
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.data_dir = Path("yahoo_ebay_data")
        self.data_dir.mkdir(exist_ok=True)
        
        print(f"âœ… çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ”‘ ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
    
    def connect_db(self):
        """PostgreSQLæ¥ç¶š"""
        try:
            conn = psycopg2.connect(**self.db_config)
            return conn
        except Exception as e:
            self.log_error(f"DBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def log_error(self, message):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        error_msg = f"[{timestamp}] ERROR: {message}"
        print(error_msg)
        
        self.session_stats['errors'].append({
            'timestamp': timestamp,
            'message': message
        })
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚è¨˜éŒ²
        log_file = self.data_dir / f"error_log_{self.session_id}.txt"
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(error_msg + "\n")
    
    def log_success(self, message):
        """æˆåŠŸãƒ­ã‚°è¨˜éŒ²"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        success_msg = f"[{timestamp}] SUCCESS: {message}"
        print(success_msg)
    
    def generate_duplicate_hash(self, title, price, image_url=""):
        """é‡è¤‡æ¤œå‡ºç”¨ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        # ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–
        normalized_title = re.sub(r'[\s\-_ã€ã€‘\[\]()ï¼ˆï¼‰]+', '', title.lower())
        
        # ä¾¡æ ¼ç¯„å›²ï¼ˆ10%ã®å¹…ã§é‡è¤‡åˆ¤å®šï¼‰
        price_range = int(price / (price * 0.1 + 1)) if price > 0 else 0
        
        # è¤‡åˆæ–‡å­—åˆ—ä½œæˆ
        composite = f"{normalized_title}_{price_range}_{image_url[:50]}"
        
        # ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        return hashlib.md5(composite.encode()).hexdigest()
    
    def generate_title_hash(self, title):
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        if not title:
            return ""
        normalized = re.sub(r'[\s\-_ã€ã€‘\[\]()ï¼ˆï¼‰]+', '', title.lower())
        return hashlib.md5(normalized.encode()).hexdigest()[:32]
    
    def check_duplicate(self, yahoo_url, title, price):
        """é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        conn = self.connect_db()
        if not conn:
            return False
        
        try:
            cursor = conn.cursor()
            
            # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
            cursor.execute(
                "SELECT product_id FROM unified_scraped_ebay_products WHERE yahoo_url = %s",
                (yahoo_url,)
            )
            
            if cursor.fetchone():
                self.log_success(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—ï¼ˆURLï¼‰: {yahoo_url}")
                self.session_stats['duplicates_skipped'] += 1
                return True
            
            # ãƒãƒƒã‚·ãƒ¥é‡è¤‡ãƒã‚§ãƒƒã‚¯
            duplicate_hash = self.generate_duplicate_hash(title, price)
            cursor.execute(
                "SELECT product_id FROM unified_scraped_ebay_products WHERE duplicate_detection_hash = %s",
                (duplicate_hash,)
            )
            
            if cursor.fetchone():
                self.log_success(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒãƒƒã‚·ãƒ¥ï¼‰: {title[:30]}...")
                self.session_stats['duplicates_skipped'] += 1
                return True
            
            return False
            
        except Exception as e:
            self.log_error(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        finally:
            if conn:
                conn.close()
    
    def scrape_yahoo_auction(self, url, debug=True):
        """
        ãƒ¤ãƒ•ã‚ªã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆçµ±åˆDBå¯¾å¿œç‰ˆï¼‰
        """
        self.log_success(f"ğŸ§ª ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
        self.session_stats['total_urls'] += 1
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=not debug)
                page = browser.new_page()
                
                # User-Agentã‚’è¨­å®š
                page.set_extra_http_headers({
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                })
                
                response = page.goto(url, timeout=30000)
                self.log_success(f"HTTP Status: {response.status}")
                
                # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                page.wait_for_timeout(3000)
                
                # Yahooå•†å“IDæŠ½å‡º
                yahoo_auction_id = ""
                auction_id_match = re.search(r'/auction/([a-zA-Z0-9]+)', url)
                if auction_id_match:
                    yahoo_auction_id = auction_id_match.group(1)
                
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                title = self.extract_title(page)
                
                # ä¾¡æ ¼å–å¾—
                price = self.extract_price(page)
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if self.check_duplicate(url, title, price):
                    return {'success': False, 'reason': 'duplicate'}
                
                # èª¬æ˜å–å¾—
                description = self.extract_description(page)
                
                # ç”»åƒURLå–å¾—
                image_urls = self.extract_images(page)
                
                # ã‚«ãƒ†ã‚´ãƒªå–å¾—
                category = self.extract_category(page)
                
                # å‡ºå“è€…æƒ…å ±å–å¾—
                seller_info = self.extract_seller_info(page)
                
                browser.close()
                
                # ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
                scraped_data = self.build_scraped_data(
                    url, yahoo_auction_id, title, description, 
                    price, image_urls, category, seller_info
                )
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                if self.save_to_database(scraped_data):
                    self.session_stats['successful_scrapes'] += 1
                    self.log_success(f"âœ… ä¿å­˜æˆåŠŸ: {title[:30]}... (Â¥{price:,})")
                    return {'success': True, 'data': scraped_data}
                else:
                    self.session_stats['failed_scrapes'] += 1
                    return {'success': False, 'reason': 'db_save_failed'}
                
        except Exception as e:
            self.log_error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            self.session_stats['failed_scrapes'] += 1
            return {'success': False, 'reason': str(e)}
    
    def extract_title(self, page):
        """ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        title_selectors = [
            'h1',
            '.ProductTitle__text',
            '[data-cl-params*="title"]',
            '.p-item-title',
            '.ProductTitle'
        ]
        
        for selector in title_selectors:
            try:
                element = page.locator(selector).first
                if element.is_visible():
                    title = element.text_content().strip()
                    if title and len(title) > 5:
                        return title
            except:
                continue
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return page.title() or "ã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—"
    
    def extract_price(self, page):
        """ä¾¡æ ¼æŠ½å‡º"""
        price_selectors = [
            'dd:has-text("å††")',
            '.Price--bid',
            '.ProductPrice dd',
            '[class*="price"]',
            '.Price'
        ]
        
        for selector in price_selectors:
            try:
                element = page.locator(selector).first
                if element.is_visible():
                    price_text = element.text_content().strip()
                    
                    # ä¾¡æ ¼å¤‰æ›
                    yen_match = re.search(r'([\d,]+)å††', price_text)
                    if yen_match:
                        return int(yen_match.group(1).replace(',', ''))
            except:
                continue
        
        return 0
    
    def extract_description(self, page):
        """èª¬æ˜æŠ½å‡º"""
        desc_selectors = [
            '.ProductExplanation__commentArea',
            '.ProductDescription__body',
            '.auct-product-desc',
            '[data-cl-params*="desc"]'
        ]
        
        for selector in desc_selectors:
            try:
                element = page.locator(selector).first
                if element.is_visible():
                    desc_text = element.text_content().strip()
                    if desc_text and len(desc_text) > 30:
                        return desc_text[:2000]  # 2000æ–‡å­—åˆ¶é™
            except:
                continue
        
        return "å•†å“èª¬æ˜ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
    
    def extract_images(self, page):
        """ç”»åƒæŠ½å‡º"""
        img_selectors = [
            '.ProductImage img',
            '.ProductImage__image',
            '[class*="ProductImage"] img',
            'img[src*="auctions.c.yimg.jp"]'
        ]
        
        image_urls = []
        for selector in img_selectors:
            try:
                imgs = page.locator(selector).all()
                for img in imgs:
                    src = img.get_attribute('src')
                    if src and self.is_valid_image_url(src):
                        image_urls.append(src)
            except:
                continue
        
        # é‡è¤‡å‰Šé™¤ã¨åˆ¶é™
        return list(dict.fromkeys(image_urls))[:12]
    
    def extract_category(self, page):
        """ã‚«ãƒ†ã‚´ãƒªæŠ½å‡º"""
        category_selectors = [
            '.p-breadcrumbs a',
            '[class*="breadcrumb"] a',
            'nav a'
        ]
        
        for selector in category_selectors:
            try:
                elements = page.locator(selector).all()
                categories = []
                for el in elements:
                    text = el.text_content().strip()
                    if text and text not in ['ãƒ›ãƒ¼ãƒ ', 'ãƒˆãƒƒãƒ—', 'TOP', 'ãƒ¤ãƒ•ã‚ªã‚¯!']:
                        categories.append(text)
                
                if categories:
                    return " > ".join(categories[-4:])  # æœ€å¾Œã®4ã¤ã®ã‚«ãƒ†ã‚´ãƒª
            except:
                continue
        
        return ""
    
    def extract_seller_info(self, page):
        """å‡ºå“è€…æƒ…å ±æŠ½å‡º"""
        seller_selectors = [
            '[class*="seller"]',
            '[class*="user"]',
            '.SellerInfo'
        ]
        
        for selector in seller_selectors:
            try:
                element = page.locator(selector).first
                if element.is_visible():
                    seller_text = element.text_content().strip()
                    if seller_text and len(seller_text) > 3:
                        return seller_text[:200]
            except:
                continue
        
        return ""
    
    def is_valid_image_url(self, url):
        """æœ‰åŠ¹ãªç”»åƒURLãƒã‚§ãƒƒã‚¯"""
        if not url or not isinstance(url, str):
            return False
        
        # ç„¡åŠ¹ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å¤–
        invalid_patterns = [
            'dsb.yahoo.co.jp', 'clear.gif', 'tracking', 'pixel', 
            'analytics', 'logo.png', 'icon', 'banner'
        ]
        
        for pattern in invalid_patterns:
            if pattern in url:
                return False
        
        # æœ‰åŠ¹ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª
        valid_patterns = [
            'auctions.c.yimg.jp', 'wing-auctions.c.yimg.jp',
            '.jpg', '.jpeg', '.png', '.gif', '.webp'
        ]
        
        return any(pattern in url for pattern in valid_patterns)
    
    def build_scraped_data(self, url, yahoo_auction_id, title, description, 
                          price, image_urls, category, seller_info):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰"""
        
        product_id = str(uuid.uuid4())[:12]  # 12æ–‡å­—ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ID
        current_time = datetime.now()
        
        # ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        title_hash = self.generate_title_hash(title)
        duplicate_hash = self.generate_duplicate_hash(
            title, price, image_urls[0] if image_urls else ""
        )
        
        return {
            # åŸºæœ¬è­˜åˆ¥
            'product_id': product_id,
            'unified_product_id': str(uuid.uuid4()),
            'scrape_timestamp': current_time,
            'yahoo_url': url,
            'yahoo_auction_id': yahoo_auction_id,
            
            # ãƒãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
            'title_hash': title_hash,
            'duplicate_detection_hash': duplicate_hash,
            
            # æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿
            'title_jp': title,
            'description_jp': description,
            'price_jpy': price,
            'category_jp': category,
            'seller_info_jp': seller_info,
            
            # ç”»åƒæƒ…å ±
            'scraped_image_urls': '|'.join(image_urls) if image_urls else '',
            'scraped_image_count': len(image_urls),
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            'status': 'scraped',
            'stock_quantity': 1,
            'scrape_success': True,
            'ebay_list_success': False,
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç®¡ç†
            'data_source_priority': 'scraped',
            'integration_status': 'scraped',
            'has_scraped_data': True,
            'has_ebay_api_data': False,
            'has_manual_data': False,
            
            # åŒæœŸãƒ•ãƒ©ã‚°
            'sync_to_tanaoroshi': True,
            'sync_to_ebay_system': False,
            'sync_to_yahoo_system': True
        }
    
    def save_to_database(self, data):
        """PostgreSQLä¿å­˜"""
        conn = self.connect_db()
        if not conn:
            return False
        
        try:
            cursor = conn.cursor()
            
            # INSERT SQL
            insert_sql = """
            INSERT INTO unified_scraped_ebay_products (
                product_id, unified_product_id, scrape_timestamp, yahoo_url, yahoo_auction_id,
                title_hash, duplicate_detection_hash, 
                title_jp, description_jp, price_jpy, category_jp, seller_info_jp,
                scraped_image_urls, scraped_image_count,
                status, stock_quantity, scrape_success, ebay_list_success,
                data_source_priority, integration_status,
                has_scraped_data, has_ebay_api_data, has_manual_data,
                sync_to_tanaoroshi, sync_to_ebay_system, sync_to_yahoo_system
            ) VALUES (
                %(product_id)s, %(unified_product_id)s, %(scrape_timestamp)s, 
                %(yahoo_url)s, %(yahoo_auction_id)s,
                %(title_hash)s, %(duplicate_detection_hash)s,
                %(title_jp)s, %(description_jp)s, %(price_jpy)s, 
                %(category_jp)s, %(seller_info_jp)s,
                %(scraped_image_urls)s, %(scraped_image_count)s,
                %(status)s, %(stock_quantity)s, %(scrape_success)s, %(ebay_list_success)s,
                %(data_source_priority)s, %(integration_status)s,
                %(has_scraped_data)s, %(has_ebay_api_data)s, %(has_manual_data)s,
                %(sync_to_tanaoroshi)s, %(sync_to_ebay_system)s, %(sync_to_yahoo_system)s
            )
            """
            
            cursor.execute(insert_sql, data)
            conn.commit()
            
            return True
            
        except Exception as e:
            self.log_error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            if conn:
                conn.rollback()
            return False
        finally:
            if conn:
                conn.close()
    
    def log_editing_history(self, product_id, edit_type, reason=""):
        """ç·¨é›†å±¥æ­´è¨˜éŒ²"""
        conn = self.connect_db()
        if not conn:
            return
        
        try:
            cursor = conn.cursor()
            
            history_data = {
                'product_id': product_id,
                'field_name': 'ebay_translation',
                'edit_type': edit_type,
                'edit_reason': reason,
                'edited_by': 'unified_scraping_system'
            }
            
            insert_sql = """
            INSERT INTO product_editing_history (
                product_id, field_name, edit_type, edit_reason, edited_by
            ) VALUES (
                %(product_id)s, %(field_name)s, %(edit_type)s, %(edit_reason)s, %(edited_by)s
            )
            """
            
            cursor.execute(insert_sql, history_data)
            conn.commit()
            
        except Exception as e:
            self.log_error(f"ç·¨é›†å±¥æ­´è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            if conn:
                conn.close()
    
    def batch_scrape_urls(self, urls, max_concurrent=3):
        """è¤‡æ•°URLä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        self.log_success(f"ğŸš€ ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {len(urls)}ä»¶")
        
        results = []
        failed_urls = []
        
        for i, url in enumerate(urls):
            self.log_success(f"é€²è¡ŒçŠ¶æ³: {i+1}/{len(urls)} - {url}")
            
            result = self.scrape_yahoo_auction(url, debug=False)
            
            if result['success']:
                results.append(result['data'])
            else:
                failed_urls.append({
                    'url': url,
                    'reason': result.get('reason', 'unknown')
                })
            
            # é–“éš”èª¿æ•´ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰
            time.sleep(2)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°ä¿å­˜
        self.save_session_log()
        
        # çµæœã‚µãƒãƒªãƒ¼
        self.log_success(f"âœ… ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†:")
        self.log_success(f"  æˆåŠŸ: {len(results)}ä»¶")
        self.log_success(f"  å¤±æ•—: {len(failed_urls)}ä»¶")
        self.log_success(f"  é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {self.session_stats['duplicates_skipped']}ä»¶")
        
        return {
            'successful_results': results,
            'failed_urls': failed_urls,
            'session_stats': self.session_stats
        }
    
    def save_session_log(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°ä¿å­˜"""
        conn = self.connect_db()
        if not conn:
            return False
        
        try:
            cursor = conn.cursor()
            
            session_data = {
                'session_id': self.session_id,
                'total_urls_processed': self.session_stats['total_urls'],
                'successful_scrapes': self.session_stats['successful_scrapes'],
                'failed_scrapes': self.session_stats['failed_scrapes'],
                'duplicate_urls_skipped': self.session_stats['duplicates_skipped'],
                'error_details': json.dumps(self.session_stats['errors'], ensure_ascii=False),
                'completed_at': datetime.now()
            }
            
            insert_sql = """
            INSERT INTO scraping_session_logs (
                session_id, total_urls_processed, successful_scrapes, 
                failed_scrapes, duplicate_urls_skipped, error_details, completed_at
            ) VALUES (
                %(session_id)s, %(total_urls_processed)s, %(successful_scrapes)s,
                %(failed_scrapes)s, %(duplicate_urls_skipped)s, %(error_details)s, %(completed_at)s
            )
            """
            
            cursor.execute(insert_sql, session_data)
            conn.commit()
            
            self.log_success("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°ä¿å­˜å®Œäº†")
            return True
            
        except Exception as e:
            self.log_error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        finally:
            if conn:
                conn.close()
    
    def get_scraped_products(self, status='scraped', limit=50):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¸ˆã¿å•†å“å–å¾—"""
        conn = self.connect_db()
        if not conn:
            return []
        
        try:
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            query = """
            SELECT 
                product_id, title_jp, description_jp, price_jpy, 
                category_jp, scraped_image_urls, yahoo_url,
                scrape_timestamp, status
            FROM unified_scraped_ebay_products 
            WHERE status = %s 
            ORDER BY scrape_timestamp DESC 
            LIMIT %s
            """
            
            cursor.execute(query, (status, limit))
            products = cursor.fetchall()
            
            return [dict(product) for product in products]
            
        except Exception as e:
            self.log_error(f"å•†å“å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
        finally:
            if conn:
                conn.close()
    
    def get_system_status(self):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—"""
        conn = self.connect_db()
        if not conn:
            return {}
        
        try:
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆå–å¾—
            cursor.execute("SELECT * FROM scraping_quality_report")
            quality_report = cursor.fetchone()
            
            # çµ±åˆçŠ¶æ³å–å¾—
            cursor.execute("SELECT * FROM integration_status_summary")
            integration_status = cursor.fetchall()
            
            # ç·¨é›†æº–å‚™å®Œäº†å•†å“æ•°
            cursor.execute("SELECT COUNT(*) as count FROM products_ready_for_editing")
            ready_for_editing = cursor.fetchone()
            
            # eBayå‡ºå“æº–å‚™å®Œäº†å•†å“æ•°
            cursor.execute("SELECT COUNT(*) as count FROM products_ready_for_ebay")
            ready_for_ebay = cursor.fetchone()
            
            return {
                'quality_report': dict(quality_report) if quality_report else {},
                'integration_status': [dict(row) for row in integration_status],
                'ready_for_editing': ready_for_editing['count'] if ready_for_editing else 0,
                'ready_for_ebay': ready_for_ebay['count'] if ready_for_ebay else 0,
                'last_check': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.log_error(f"ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
        finally:
            if conn:
                conn.close()

# ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œç”¨
def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python unified_scraping_system.py <URL>")
        print("  python unified_scraping_system.py batch <URL1> <URL2> <URL3>...")
        print("  python unified_scraping_system.py status")
        return
    
    scraper = UnifiedScrapingSystem()
    
    command = sys.argv[1]
    
    if command == 'status':
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
        status = scraper.get_system_status()
        print(json.dumps(status, indent=2, ensure_ascii=False))
    
    elif command == 'batch':
        # ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        urls = sys.argv[2:]
        if not urls:
            print("âŒ URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            return
        
        results = scraper.batch_scrape_urls(urls)
        print(f"âœ… ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(results['successful_results'])}ä»¶æˆåŠŸ")
    
    else:
        # å˜ä¸€URL ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        url = sys.argv[1]
        result = scraper.scrape_yahoo_auction(url)
        
        if result['success']:
            print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {result['data']['title_jp']}")
        else:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {result['reason']}")

if __name__ == "__main__":
    main()
