# main.py
import os
import asyncio
from datetime import datetime, timedelta
from celery import Celery
from celery.schedules import crontab
from fastapi import FastAPI, BackgroundTasks
from workflow_engines import automated_processing_pipeline, DistributedScraper
import asyncpg
import pandas as pd

# FastAPIã¨Celeryã®åˆæœŸåŒ–
app = FastAPI()
celery_app = Celery('tasks', broker=os.getenv('REDIS_URL'), backend=os.getenv('REDIS_URL'))
celery_app.conf.update(
    task_track_started=True
)

# Celeryå®šæœŸå®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è¨­å®š
celery_app.conf.beat_schedule = {
    'daily-inventory-update': {
        'task': 'main.update_inventory_prices',
        'schedule': crontab(hour=2, minute=0),
    },
    'scrape-new-products': {
        'task': 'main.scrape_new_yahoo_products',
        'schedule': crontab(minute=0, hour='*/6'),
    }
}

async def get_db_conn():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®ç¢ºç«‹"""
    return await asyncpg.connect(os.getenv('DATABASE_URL'))

# Celeryã‚¿ã‚¹ã‚¯ã®å®šç¾©
@celery_app.task(name='main.update_inventory_prices')
def update_inventory_prices():
    """åœ¨åº«ãƒ»ä¾¡æ ¼ã®è‡ªå‹•æ›´æ–°"""
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ä¾¡æ ¼å¤‰å‹•ã‚„åœ¨åº«åˆ‡ã‚Œã‚’æ¤œçŸ¥
    # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼å‡¦ç†
    print("ğŸ”„ ä¾¡æ ¼ãƒ»åœ¨åº«ã®è‡ªå‹•æ›´æ–°ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œä¸­...")
    time.sleep(5)
    print("âœ… ä¾¡æ ¼ãƒ»åœ¨åº«æ›´æ–°ã‚¿ã‚¹ã‚¯å®Œäº†ã€‚")
    return "ä¾¡æ ¼æ›´æ–°å®Œäº†: 0ä»¶ (ãƒ€ãƒŸãƒ¼)"

@celery_app.task(name='main.scrape_new_yahoo_products')
def scrape_new_yahoo_products():
    """æ–°å•†å“ã®è‡ªå‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    print("ğŸ”„ æ–°å•†å“ã®è‡ªå‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
    urls = [f'https://yahoo.jp/auction/new/{i}' for i in range(100)]
    
    scraper = DistributedScraper()
    results = asyncio.run(scraper.scrape_with_load_balancing(urls))
    
    # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼å‡¦ç†
    print(f"âœ… æ–°å•†å“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¿ã‚¹ã‚¯å®Œäº†ã€‚{len(results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return f"æ–°å•†å“å–å¾—å®Œäº†: {len(results)}ä»¶"

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.post("/run_pipeline/{product_id}")
async def run_pipeline(product_id: str, background_tasks: BackgroundTasks):
    """æ‰‹å‹•ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒˆãƒªã‚¬ãƒ¼"""
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
    product_data = {
        'product_id': product_id,
        'title_jp': 'ãƒ†ã‚¹ãƒˆå•†å“ã‚¿ã‚¤ãƒˆãƒ«',
        'description_jp': 'ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®å•†å“èª¬æ˜ã§ã™ã€‚',
        'image_urls': 'https://example.com/image.jpg',
        'price_jpy': 10000,
        'condition': 'ä¸­å¤'
    }
    
    # Celeryã‚¿ã‚¹ã‚¯ã¨ã—ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
    # background_tasks.add_task(automated_processing_pipeline_task.delay, product_data)
    
    return {"message": f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹ã•ã‚Œã¾ã—ãŸ: {product_id}"}

@celery_app.task
def automated_processing_pipeline_task(product_data: Dict):
    """è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’Celeryã‚¿ã‚¹ã‚¯ã¨ã—ã¦å®Ÿè¡Œ"""
    return asyncio.run(automated_processing_pipeline(product_data))