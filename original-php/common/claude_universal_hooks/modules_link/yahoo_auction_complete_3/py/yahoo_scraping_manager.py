#!/usr/bin/env python3
"""
çµ±åˆYahooã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ï¼ˆç¬¬ä¸€éšå±¤ç‰ˆï¼‰
"""

import sys
import os
sys.path.append('/Users/aritahiroaki/NAGANO-3/N3-Development/yahoo_scraping_tools')

from yahoo_auction_scraper import scrape_auction_data, scrape_paypay_fleamarket
from multi_site_scraper import MultiSiteScraper
import sqlite3
import time
from datetime import datetime
import json

class YahooScrapingManager:
    def __init__(self, database_path='/Users/aritahiroaki/NAGANO-3/N3-Development/modules/yahoo_auction_complete/yahoo_auction_tool/yahoo_ebay_workflow_enhanced.db'):
        self.database_path = database_path
        self.multi_scraper = MultiSiteScraper()
        
    def scrape_and_save(self, urls, use_playwright=True):
        """
        URLãƒªã‚¹ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        """
        results = []
        success_count = 0
        
        for url in urls:
            try:
                # URLã®ç¨®é¡åˆ¤å®š
                if 'auctions.yahoo.co.jp' in url:
                    if use_playwright:
                        # Playwrightç‰ˆï¼ˆé«˜ç²¾åº¦ï¼‰
                        data = scrape_auction_data(url)
                    else:
                        # BeautifulSoupç‰ˆï¼ˆè»½é‡ï¼‰
                        data = self.multi_scraper.scrape_yahoo_auction(url)
                elif 'paypayfleamarket.yahoo.co.jp' in url:
                    data = scrape_paypay_fleamarket(url)
                else:
                    data = None
                    print(f"æœªå¯¾å¿œã®URL: {url}")
                
                if data:
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                    product_id = self.save_to_database(data, url)
                    data['product_id'] = product_id
                    results.append(data)
                    success_count += 1
                    print(f"âœ… æˆåŠŸ: {data.get('title_jp', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')}")
                else:
                    results.append({'url': url, 'error': 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—'})
                    print(f"âŒ å¤±æ•—: {url}")
                
                # 1ç§’å¾…æ©Ÿï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰
                time.sleep(1)
                
            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                results.append({'url': url, 'error': str(e)})
        
        return {
            'total_urls': len(urls),
            'success_count': success_count,
            'failed_count': len(urls) - success_count,
            'results': results
        }
    
    def save_to_database(self, data, source_url):
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        """
        conn = sqlite3.connect(self.database_path)
        cursor = conn.cursor()
        
        # ä¸€æ„ã®product_idç”Ÿæˆ
        timestamp = int(time.time() * 1000)  # ãƒŸãƒªç§’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        product_id = f"SCRAPED_{timestamp}"
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥
        cursor.execute('''
            INSERT INTO products_enhanced 
            (product_id, title_jp, title_en, description_jp, price_jpy, 
             main_image_url, source_url, source_platform, condition_jp, 
             status, data_quality_score, scraped_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            product_id,
            data.get('title_jp', ''),
            self.translate_title(data.get('title_jp', '')),  # ç°¡æ˜“è‹±è¨³
            data.get('description_jp', ''),
            data.get('price_jpy', 0),
            data.get('image_urls', '').split('|')[0] if data.get('image_urls') else '',
            source_url,
            'yahoo',
            data.get('condition', 'ä¸­å¤'),
            'scraped',
            0.9,  # å®Ÿãƒ‡ãƒ¼ã‚¿ãªã®ã§å“è³ªã‚¹ã‚³ã‚¢é«˜ã‚
            datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        ))
        
        conn.commit()
        conn.close()
        
        return product_id
    
    def translate_title(self, japanese_title):
        """
        ç°¡æ˜“çš„ãªè‹±è¨³ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        """
        translation_map = {
            'æ–°å“': 'New',
            'ä¸­å¤': 'Used',
            'ç¾å“': 'Excellent',
            'iPhone': 'iPhone',
            'iPad': 'iPad',
            'MacBook': 'MacBook',
            'ã‚²ãƒ¼ãƒ ': 'Game',
            'ãƒ•ã‚£ã‚®ãƒ¥ã‚¢': 'Figure',
            'æœ¬': 'Book',
            'æ™‚è¨ˆ': 'Watch',
            'ã‚«ãƒ¡ãƒ©': 'Camera',
            'è»Š': 'Car',
            'ãƒã‚¤ã‚¯': 'Motorcycle'
        }
        
        english_title = japanese_title
        for jp, en in translation_map.items():
            english_title = english_title.replace(jp, en)
        
        return english_title[:100]  # 100æ–‡å­—åˆ¶é™

def main():
    """
    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python yahoo_scraping_manager.py <URL1> [URL2] [URL3] ...")
        print("ä¾‹: python yahoo_scraping_manager.py https://auctions.yahoo.co.jp/jp/auction/example123")
        return
    
    urls = sys.argv[1:]
    manager = YahooScrapingManager()
    
    print(f"ğŸš€ {len(urls)}ä»¶ã®URLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
    results = manager.scrape_and_save(urls)
    
    print("\nğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ:")
    print(f"   ç·æ•°: {results['total_urls']}ä»¶")
    print(f"   æˆåŠŸ: {results['success_count']}ä»¶")
    print(f"   å¤±æ•—: {results['failed_count']}ä»¶")
    
    if results['success_count'] > 0:
        print("\nâœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸï¼")
        print("   ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ã€Œãƒ‡ãƒ¼ã‚¿èª­è¾¼ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == '__main__':
    main()
